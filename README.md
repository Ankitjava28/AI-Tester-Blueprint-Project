# ðŸ§ª Local LLM Test Case Generator

> A privacy-focused, zero-dependency AI assistant that generates professional test cases using your local machine.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)
![Architecture](https://img.shields.io/badge/architecture-A.N.T.-purple.svg)

---

## ðŸ—ï¸ Architecture

This project follows the **A.N.T. (API, Negotiation, Terminal)** 3-Layer Architecture.

```mermaid
graph TD
    User[ðŸ‘¤ User] -->|1. Enters Requirement| UI[ðŸ–¥ï¸ Frontend Terminal (HTML/JS)]
    UI -->|2. POST /api/chat| API[âš™ï¸ Backend API Layer (Node.js Native)]
    
    subgraph "Layer 2: Negotiation"
        API -->|3. formatPrompt()| Template[ðŸ“ Prompt Engine]
        Template -->|4. Stream Request| OllamaService[ðŸ¤– Ollama Service]
    end
    
    subgraph "Layer 3: Tools & Inference"
        OllamaService -->|5. HTTP Requests| LocalLLM[ðŸ¦™ Ollama (llama3.2:3b)]
    end
    
    LocalLLM -->|6. Streaming Tokens| OllamaService
    OllamaService -->|7. Chunked Response| UI
```

---

## ðŸŒŸ Key Features

- **ðŸ”’ 100% Local**: No data leaves your machine. Powered by Ollama.
- **âš¡ Zero Dependencies**: Backend runs on native Node.js (no `npm install` hell for the server).
- **ðŸŽ¨ Premium UI**: Dark mode, glassmorphism, and markdown rendering.
- **ðŸ“‹ Copy-Ready**: One-click copy for generated test tables/scripts.
- **ðŸ§  Deterministic Logic**: Uses specific SOPs (Standard Operating Procedures) for consistent outputs.

---

## ðŸ› ï¸ Prerequisites

1.  **Node.js** (v18 or higher)
2.  **Ollama**: [Download and Install](https://ollama.com/)
3.  **Model**: Pull the required model:
    ```bash
    ollama pull llama3.2:3b
    ```

---

## ðŸš€ How to Run

1.  **Start Ollama** (if not running):
    ```bash
    ollama serve
    ```

2.  **Start the Backend**:
    ```bash
    node backend/server.js
    ```
    *You should see:* `ðŸš€ A.N.T. Backend System Online...`

3.  **Open the App**:
    Go to **[http://localhost:3001](http://localhost:3001)** in your browser.

---

## ðŸ“‚ Project Structure

```
â”œâ”€â”€ architecture/         # Layer 1: SOPs and Design Docs
â”‚   â””â”€â”€ sop_llm_interaction.md
â”œâ”€â”€ backend/              # Layer 2: API & Logic
â”‚   â”œâ”€â”€ server.js         # Native HTTP Server
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ ollamaService.js # Template Engine
â”œâ”€â”€ index.html            # Layer 3: Frontend (Single File)
â”œâ”€â”€ tools/                # Utility Scripts
â”‚   â””â”€â”€ test_ollama.js    # Connection Verification
â”œâ”€â”€ BLAST.md              # Project Blueprint & Protocol
â””â”€â”€ gemini.md             # Project Constitution & Schemas
```

---

## ðŸ¤– Prompt Template

The core intelligence uses a structured template to ensure quality:
> "You remain a QA Automation Expert. Generate detailed test cases for the following requirement..."

---

## ðŸ“„ License

This project is licensed under the MIT License.
